RAG stands for Retrieval Augmented Generation.

A RAG chatbot combines a Large Language Model (LLM) with an external knowledge base.
Instead of relying only on its training data, the chatbot retrieves relevant documents
from a vector database and uses them to generate accurate answers.

The RAG pipeline has three main steps:
1. Ingestion – documents are loaded, split into chunks, and converted into embeddings.
2. Retrieval – relevant chunks are fetched based on the user query.
3. Generation – the LLM generates an answer using the retrieved context.

RAG is useful for building chatbots that answer questions from private data such as
PDFs, company documents, notes, or databases.
